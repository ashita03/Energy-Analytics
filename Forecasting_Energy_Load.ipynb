{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k49cYPbzR8EH"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pickle\n",
        "import joblib\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Set style for visualizations\n",
        "#plt.style.use('seaborn')\n",
        "sns.set_palette(\"husl\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaIXPXSDSCa4",
        "outputId": "cf2a39a9-5d55-4e0c-c61f-d403a9f44d3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value    0.106426\n",
            "Name: 24, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "data = {'index': list(range(50)), 'value': np.random.rand(50)}\n",
        "df = pd.DataFrame(data).set_index('index')\n",
        "\n",
        "def safe_access(df, index):\n",
        "    \"\"\"Safely access a row in a DataFrame without causing a KeyError.\"\"\"\n",
        "    if index in df.index:\n",
        "        return df.loc[index]\n",
        "    else:\n",
        "        print(f\"Warning: Index {index} not found in DataFrame.\")\n",
        "        return None\n",
        "\n",
        "# Example usage in your script\n",
        "index_to_access = 24\n",
        "result = safe_access(df, index_to_access)\n",
        "\n",
        "if result is not None:\n",
        "    # Proceed with processing result\n",
        "    print(result)\n",
        "\n",
        "class EnhancedLSTMForecaster:\n",
        "    def __init__(self, sequence_length=24, n_features=None):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.n_features = n_features\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.history = None\n",
        "\n",
        "    def create_sequences(self, X, y=None):\n",
        "        \"\"\"Create sequences for LSTM input with consistent lengths\"\"\"\n",
        "        sequences_X = []\n",
        "        sequences_y = []\n",
        "\n",
        "        # Convert to numpy array if DataFrame\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.values\n",
        "        if y is not None and isinstance(y, pd.Series):\n",
        "            y = y.values\n",
        "\n",
        "        # Create sequences ensuring consistent lengths\n",
        "        for i in range(len(X) - self.sequence_length):\n",
        "            seq_x = X[i:(i + self.sequence_length)]\n",
        "            sequences_X.append(seq_x)\n",
        "\n",
        "            if y is not None:\n",
        "                sequences_y.append(y[i + self.sequence_length])\n",
        "\n",
        "        sequences_X = np.array(sequences_X)\n",
        "\n",
        "        if y is not None:\n",
        "            sequences_y = np.array(sequences_y)\n",
        "            # Ensure X and y have same number of samples\n",
        "            min_len = min(len(sequences_X), len(sequences_y))\n",
        "            return sequences_X[:min_len], sequences_y[:min_len]\n",
        "\n",
        "        return sequences_X\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Build an enhanced LSTM model architecture\"\"\"\n",
        "        model = Sequential([\n",
        "            Bidirectional(LSTM(128, return_sequences=True),\n",
        "                         input_shape=(self.sequence_length, self.n_features)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            Bidirectional(LSTM(64, return_sequences=True)),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.3),\n",
        "\n",
        "            LSTM(32),\n",
        "            BatchNormalization(),\n",
        "            Dropout(0.2),\n",
        "\n",
        "            Dense(16, activation='relu'),\n",
        "            BatchNormalization(),\n",
        "            Dense(8, activation='relu'),\n",
        "            Dense(1)\n",
        "        ])\n",
        "\n",
        "        optimizer = Adam(learning_rate=0.001)\n",
        "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32):\n",
        "        \"\"\"Train the LSTM model with consistent sequence lengths\"\"\"\n",
        "        # Create sequences\n",
        "        print(\"Creating training sequences...\")\n",
        "        X_train_seq, y_train_seq = self.create_sequences(X_train, y_train)\n",
        "        print(f\"Training sequences shape: X={X_train_seq.shape}, y={y_train_seq.shape}\")\n",
        "\n",
        "        print(\"Creating validation sequences...\")\n",
        "        X_val_seq, y_val_seq = self.create_sequences(X_val, y_val)\n",
        "        print(f\"Validation sequences shape: X={X_val_seq.shape}, y={y_val_seq.shape}\")\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=10,\n",
        "                restore_best_weights=True,\n",
        "                mode='min'\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=5,\n",
        "                min_lr=0.00001,\n",
        "                mode='min'\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            X_train_seq, y_train_seq,\n",
        "            validation_data=(X_val_seq, y_val_seq),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return self.history\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions with proper sequence handling\"\"\"\n",
        "        # Create sequences without labels\n",
        "        X_seq = self.create_sequences(X)\n",
        "        predictions = self.model.predict(X_seq)\n",
        "\n",
        "        # Create padded predictions array\n",
        "        padded_predictions = np.full(len(X), np.nan)\n",
        "        padded_predictions[self.sequence_length:len(X_seq) + self.sequence_length] = predictions.flatten()\n",
        "\n",
        "        return padded_predictions\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"Plot training history\"\"\"\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.history.history['loss'], label='Training Loss')\n",
        "        plt.plot(self.history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Model Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.history.history['mae'], label='Training MAE')\n",
        "        plt.plot(self.history.history['val_mae'], label='Validation MAE')\n",
        "        plt.title('Model MAE')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('MAE')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('eda_plots/lstm_training_history.png')\n",
        "        plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Pz5DLOMZSJj7"
      },
      "outputs": [],
      "source": [
        "# Load and prepare data functions remain the same\n",
        "def load_data(train_path, test_path):\n",
        "    train_df = pd.read_excel(train_path)\n",
        "    test_df = pd.read_excel(test_path)\n",
        "\n",
        "    numeric_columns = ['Year', 'Month', 'Day', 'Hour', 'Load'] + \\\n",
        "                     [col for col in train_df.columns if 'Temp' in col or 'GHI' in col]\n",
        "    for col in numeric_columns:\n",
        "        if col in train_df.columns:\n",
        "            train_df[col] = pd.to_numeric(train_df[col], errors='coerce')\n",
        "        if col in test_df.columns:\n",
        "            test_df[col] = pd.to_numeric(test_df[col], errors='coerce')\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "def create_datetime_features(df):\n",
        "    \"\"\"Create datetime features from relative Year (1,2,3), Month, Day, Hour columns\"\"\"\n",
        "    base_year = 2020\n",
        "    df['actual_year'] = base_year + df['Year'] - 1\n",
        "\n",
        "    date_components = ['actual_year', 'Month', 'Day', 'Hour']\n",
        "    for col in date_components:\n",
        "        df[col] = df[col].astype(int)\n",
        "\n",
        "    df['datetime'] = pd.to_datetime({\n",
        "        'year': df['actual_year'],\n",
        "        'month': df['Month'],\n",
        "        'day': df['Day'],\n",
        "        'hour': df['Hour']\n",
        "    })\n",
        "\n",
        "    df['dayofweek'] = df['datetime'].dt.dayofweek\n",
        "    df['quarter'] = df['datetime'].dt.quarter\n",
        "    df['month'] = df['datetime'].dt.month\n",
        "    df['hour'] = df['datetime'].dt.hour\n",
        "    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
        "    df['day_of_year'] = df['datetime'].dt.dayofyear\n",
        "\n",
        "    df['season'] = pd.cut(df['month'],\n",
        "                         bins=[0, 3, 6, 9, 12],\n",
        "                         labels=['Winter', 'Spring', 'Summer', 'Fall'])\n",
        "\n",
        "    df = df.drop('actual_year', axis=1)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "44FLrZhiS8u5"
      },
      "outputs": [],
      "source": [
        "def perform_eda(train_df):\n",
        "    \"\"\"Perform Exploratory Data Analysis\"\"\"\n",
        "    # Create directory for plots if it doesn't exist\n",
        "    import os\n",
        "    if not os.path.exists('eda_plots'):\n",
        "        os.makedirs('eda_plots')\n",
        "\n",
        "    # 1. Time Series Analysis\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.plot(train_df['datetime'], train_df['Load'])\n",
        "    plt.title('Load Over Time')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('eda_plots/load_time_series.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Distribution Analysis\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.histplot(train_df['Load'], bins=50)\n",
        "    plt.title('Load Distribution')\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.boxplot(x='season', y='Load', data=train_df)\n",
        "    plt.title('Load by Season')\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    sns.boxplot(x='hour', y='Load', data=train_df)\n",
        "    plt.title('Load by Hour')\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    sns.boxplot(x='dayofweek', y='Load', data=train_df)\n",
        "    plt.title('Load by Day of Week')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('eda_plots/load_distributions.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Correlation Analysis\n",
        "    temp_cols = [col for col in train_df.columns if 'Temp' in col]\n",
        "    ghi_cols = [col for col in train_df.columns if 'GHI' in col]\n",
        "    features_for_corr = ['Load'] + temp_cols + ghi_cols\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    correlation = train_df[features_for_corr].corr()\n",
        "    sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "    plt.title('Correlation Heatmap')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('eda_plots/correlation_heatmap.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\nSummary Statistics for Load:\")\n",
        "    print(train_df['Load'].describe())\n",
        "\n",
        "    # Temporal patterns\n",
        "    print(\"\\nAverage Load by Season:\")\n",
        "    print(train_df.groupby('season')['Load'].mean())\n",
        "\n",
        "    print(\"\\nAverage Load by Hour:\")\n",
        "    print(train_df.groupby('hour')['Load'].mean())\n",
        "\n",
        "    # Return some key statistics for use in modeling\n",
        "    return {\n",
        "        'load_mean': train_df['Load'].mean(),\n",
        "        'load_std': train_df['Load'].std(),\n",
        "        'load_min': train_df['Load'].min(),\n",
        "        'load_max': train_df['Load'].max()\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2ao_JVICTW7-"
      },
      "outputs": [],
      "source": [
        "def prepare_features(df, is_training=True):\n",
        "    \"\"\"Prepare features for modeling\"\"\"\n",
        "    # Calculate rolling means for temperature and GHI\n",
        "    temp_cols = [col for col in df.columns if 'Temp' in col]\n",
        "    ghi_cols = [col for col in df.columns if 'GHI' in col]\n",
        "\n",
        "    # Sort by datetime to ensure correct rolling calculations\n",
        "    df = df.sort_values('datetime')\n",
        "\n",
        "    for col in temp_cols + ghi_cols:\n",
        "        df[f'{col}_rolling_mean_24h'] = df[col].rolling(window=24, min_periods=1).mean()\n",
        "        df[f'{col}_rolling_mean_7d'] = df[col].rolling(window=24*7, min_periods=1).mean()\n",
        "\n",
        "    # Create lag features for Load (only for training data)\n",
        "    if 'Load' in df.columns and is_training:\n",
        "        df['Load_lag_1h'] = df['Load'].shift(1)\n",
        "        df['Load_lag_24h'] = df['Load'].shift(24)\n",
        "        df['Load_lag_7d'] = df['Load'].shift(24*7)\n",
        "\n",
        "        # Rolling statistics for Load\n",
        "        df['Load_rolling_mean_24h'] = df['Load'].rolling(window=24, min_periods=1).mean()\n",
        "        df['Load_rolling_std_24h'] = df['Load'].rolling(window=24, min_periods=1).std()\n",
        "\n",
        "    # Drop rows with NaN values created by lag features\n",
        "    if is_training:\n",
        "        df = df.dropna()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XOAgnGmySPh8"
      },
      "outputs": [],
      "source": [
        "class LoadForecaster:\n",
        "    def __init__(self):\n",
        "        self.models = {\n",
        "            'linear': LinearRegression(),\n",
        "            'rf': RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "            'gbm': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "            'xgb': XGBRegressor(n_estimators=100, random_state=42),\n",
        "            'lstm': None\n",
        "        }\n",
        "        self.scaler = StandardScaler()\n",
        "        self.feature_cols = None\n",
        "\n",
        "    def train_evaluate_models(self, X_train, y_train, X_val, y_val):\n",
        "        results = {}\n",
        "        self.feature_cols = X_train.columns\n",
        "\n",
        "        # Scale the features\n",
        "        X_train_scaled = pd.DataFrame(\n",
        "            self.scaler.fit_transform(X_train),\n",
        "            columns=X_train.columns,\n",
        "            index=X_train.index\n",
        "        )\n",
        "        X_val_scaled = pd.DataFrame(\n",
        "            self.scaler.transform(X_val),\n",
        "            columns=X_val.columns,\n",
        "            index=X_val.index\n",
        "        )\n",
        "\n",
        "        # Train traditional models\n",
        "        for name, model in self.models.items():\n",
        "            if name != 'lstm':\n",
        "                print(f\"\\nTraining {name} model...\")\n",
        "                model.fit(X_train_scaled, y_train)\n",
        "                y_pred = model.predict(X_val_scaled)\n",
        "                results[name] = self.calculate_metrics(y_val, y_pred)\n",
        "                self.plot_predictions(y_val, y_pred, name)\n",
        "\n",
        "        # Train LSTM model\n",
        "        print(\"\\nTraining LSTM model...\")\n",
        "        lstm_forecaster = EnhancedLSTMForecaster(\n",
        "            sequence_length=24,\n",
        "            n_features=X_train.shape[1]\n",
        "        )\n",
        "        lstm_forecaster.build_model()\n",
        "\n",
        "        # Make sure data is properly aligned before sequence creation\n",
        "        X_train_seq = X_train_scaled.copy()\n",
        "        y_train_seq = y_train.copy()\n",
        "        X_val_seq = X_val_scaled.copy()\n",
        "        y_val_seq = y_val.copy()\n",
        "\n",
        "        # Print shapes before sequence creation\n",
        "        print(f\"Before sequence creation - X_train: {X_train_seq.shape}, y_train: {y_train_seq.shape}\")\n",
        "\n",
        "        history = lstm_forecaster.train(\n",
        "            X_train_seq, y_train_seq,\n",
        "            X_val_seq, y_val_seq,\n",
        "            epochs=100,\n",
        "            batch_size=32\n",
        "        )\n",
        "\n",
        "        lstm_predictions = lstm_forecaster.predict(X_val_seq)\n",
        "        valid_predictions = lstm_predictions[~np.isnan(lstm_predictions)]\n",
        "        valid_targets = y_val.iloc[len(y_val)-len(valid_predictions):]\n",
        "\n",
        "        results['lstm'] = self.calculate_metrics(valid_targets, valid_predictions)\n",
        "        self.plot_predictions(valid_targets, valid_predictions, 'lstm')\n",
        "\n",
        "        lstm_forecaster.plot_training_history()\n",
        "\n",
        "        return results\n",
        "\n",
        "    def calculate_metrics(self, y_true, y_pred):\n",
        "        return {\n",
        "            'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "            'MAE': mean_absolute_error(y_true, y_pred),\n",
        "            'R2': r2_score(y_true, y_pred)\n",
        "        }\n",
        "\n",
        "    def plot_predictions(self, y_true, y_pred, model_name):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(y_true.values[:100], label='Actual')\n",
        "        plt.plot(y_pred[:100], label='Predicted')\n",
        "        plt.title(f'{model_name.upper()} Model - Actual vs Predicted (First 100 points)')\n",
        "        plt.legend()\n",
        "        plt.savefig(f'eda_plots/{model_name}_predictions.png')\n",
        "        plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_trained_models(forecaster, base_path='saved_models'):\n",
        "      \"\"\"Save trained models from a LoadForecaster instance.\n",
        "\n",
        "      Args:\n",
        "          forecaster (LoadForecaster): Instance of LoadForecaster with trained models\n",
        "          base_path (str): Base directory to save the models\n",
        "      \"\"\"\n",
        "      # Create timestamp for versioning\n",
        "      timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "      # Create base directory if it doesn't exist\n",
        "      model_path = os.path.join(base_path, timestamp)\n",
        "      os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "      # Save feature columns\n",
        "      if forecaster.feature_cols is not None:\n",
        "          joblib.dump(forecaster.feature_cols, os.path.join(model_path, 'feature_cols.joblib'))\n",
        "\n",
        "      # Save scaler\n",
        "      joblib.dump(forecaster.scaler, os.path.join(model_path, 'scaler.joblib'))\n",
        "\n",
        "      # Save traditional ML models\n",
        "      for name, model in forecaster.models.items():\n",
        "          if name != 'lstm' and model is not None:\n",
        "              joblib.dump(model, os.path.join(model_path, f'{name}_model.joblib'))\n",
        "\n",
        "      # Save LSTM model if it exists\n",
        "      if forecaster.models['lstm'] is not None:\n",
        "          lstm_path = os.path.join(model_path, 'lstm_model')\n",
        "          os.makedirs(lstm_path, exist_ok=True)\n",
        "\n",
        "          # Save model architecture and weights\n",
        "          model_json = forecaster.models['lstm'].model.to_json()\n",
        "          with open(os.path.join(lstm_path, 'model_architecture.json'), 'w') as f:\n",
        "              f.write(model_json)\n",
        "\n",
        "          # Save weights\n",
        "          forecaster.models['lstm'].model.save_weights(os.path.join(lstm_path, 'model_weights.h5'))\n",
        "\n",
        "          # Save sequence length and other LSTM-specific parameters\n",
        "          lstm_params = {\n",
        "              'sequence_length': forecaster.models['lstm'].sequence_length,\n",
        "              'n_features': forecaster.models['lstm'].n_features\n",
        "          }\n",
        "          joblib.dump(lstm_params, os.path.join(lstm_path, 'lstm_params.joblib'))\n",
        "\n",
        "      print(f\"Models saved successfully in {model_path}\")\n",
        "      return model_path"
      ],
      "metadata": {
        "id": "n5AXUw-JXzjQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rR9oimiSQ1h",
        "outputId": "bcf867a6-10f4-465f-fd0a-bc64985dba9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Creating datetime features...\n",
            "Performing EDA...\n",
            "\n",
            "Summary Statistics for Load:\n",
            "count    17544.000000\n",
            "mean      2154.130529\n",
            "std        437.248479\n",
            "min       1027.000000\n",
            "25%       1847.000000\n",
            "50%       2085.000000\n",
            "75%       2392.000000\n",
            "max       4397.000000\n",
            "Name: Load, dtype: float64\n",
            "\n",
            "Average Load by Season:\n",
            "season\n",
            "Winter    2016.964088\n",
            "Spring    1931.864469\n",
            "Summer    2488.539176\n",
            "Fall      2174.502038\n",
            "Name: Load, dtype: float64\n",
            "\n",
            "Average Load by Hour:\n",
            "hour\n",
            "0     2039.465116\n",
            "1     1951.441860\n",
            "2     1885.556772\n",
            "3     1851.239398\n",
            "4     1859.811218\n",
            "5     1922.331053\n",
            "6     2023.117647\n",
            "7     2114.181943\n",
            "8     2124.704514\n",
            "9     2053.954856\n",
            "10    1969.744186\n",
            "11    1920.242134\n",
            "12    1917.149111\n",
            "13    1959.822161\n",
            "14    2047.986320\n",
            "15    2187.968536\n",
            "16    2352.477428\n",
            "17    2535.569083\n",
            "18    2666.336525\n",
            "19    2687.476060\n",
            "20    2626.852257\n",
            "21    2504.279070\n",
            "22    2335.606019\n",
            "23    2161.819425\n",
            "Name: Load, dtype: float64\n",
            "Preparing features...\n",
            "Training set shapes - X: (13900, 41), y: (13900,)\n",
            "Validation set shapes - X: (3476, 41), y: (3476,)\n",
            "Training models...\n",
            "\n",
            "Training linear model...\n",
            "\n",
            "Training rf model...\n",
            "\n",
            "Training gbm model...\n",
            "\n",
            "Training xgb model...\n",
            "\n",
            "Training LSTM model...\n",
            "Before sequence creation - X_train: (13900, 41), y_train: (13900,)\n",
            "Creating training sequences...\n",
            "Training sequences shape: X=(13876, 24, 41), y=(13876,)\n",
            "Creating validation sequences...\n",
            "Validation sequences shape: X=(3452, 24, 41), y=(3452,)\n",
            "Epoch 1/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 92ms/step - loss: 4641312.5000 - mae: 2110.3142 - val_loss: 4843139.5000 - val_mae: 2166.6719 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 93ms/step - loss: 4042167.7500 - mae: 1968.8617 - val_loss: 3082238.2500 - val_mae: 1719.2253 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 98ms/step - loss: 2454950.5000 - mae: 1518.6011 - val_loss: 1059887.1250 - val_mae: 993.8420 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 92ms/step - loss: 877665.1250 - mae: 842.0162 - val_loss: 206710.3906 - val_mae: 392.1555 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 90ms/step - loss: 262534.5938 - mae: 378.5439 - val_loss: 36337.9570 - val_mae: 141.9229 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 89ms/step - loss: 150253.0469 - mae: 281.5493 - val_loss: 76103.0781 - val_mae: 206.0878 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 93ms/step - loss: 199446.0312 - mae: 317.5384 - val_loss: 49699.6016 - val_mae: 162.4999 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 90ms/step - loss: 151092.6094 - mae: 285.1299 - val_loss: 35697.5039 - val_mae: 142.9358 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - loss: 109349.5469 - mae: 242.8798 - val_loss: 40720.2070 - val_mae: 153.3526 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 92ms/step - loss: 101923.5703 - mae: 234.8667 - val_loss: 36783.4727 - val_mae: 144.2607 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - loss: 95531.8516 - mae: 221.7935 - val_loss: 27838.7852 - val_mae: 123.7852 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 96ms/step - loss: 92510.5391 - mae: 223.5956 - val_loss: 75660.3438 - val_mae: 227.6006 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 91ms/step - loss: 76543.0625 - mae: 206.3304 - val_loss: 40155.0000 - val_mae: 157.2514 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 91ms/step - loss: 55499.7773 - mae: 179.2182 - val_loss: 41958.5547 - val_mae: 158.3484 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - loss: 62076.5156 - mae: 187.9483 - val_loss: 37778.5508 - val_mae: 150.8836 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 93ms/step - loss: 55382.9492 - mae: 181.9061 - val_loss: 46690.4609 - val_mae: 162.3546 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 91ms/step - loss: 76313.3203 - mae: 205.0028 - val_loss: 39337.1406 - val_mae: 151.2240 - learning_rate: 5.0000e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 92ms/step - loss: 62219.3320 - mae: 190.1684 - val_loss: 33379.1328 - val_mae: 137.0429 - learning_rate: 5.0000e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 90ms/step - loss: 54010.7070 - mae: 177.9913 - val_loss: 30828.3613 - val_mae: 131.6544 - learning_rate: 5.0000e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 94ms/step - loss: 46615.3945 - mae: 165.4547 - val_loss: 30164.5840 - val_mae: 132.4752 - learning_rate: 5.0000e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 91ms/step - loss: 44320.7812 - mae: 161.9696 - val_loss: 26667.5254 - val_mae: 122.8583 - learning_rate: 5.0000e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 92ms/step - loss: 42311.4883 - mae: 158.9115 - val_loss: 26872.6582 - val_mae: 121.9470 - learning_rate: 5.0000e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - loss: 38617.6367 - mae: 151.6396 - val_loss: 26779.1699 - val_mae: 120.6526 - learning_rate: 5.0000e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 93ms/step - loss: 36137.9453 - mae: 146.4000 - val_loss: 24869.6035 - val_mae: 115.0931 - learning_rate: 5.0000e-04\n",
            "Epoch 25/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 99ms/step - loss: 37343.6914 - mae: 148.2238 - val_loss: 26169.8867 - val_mae: 116.9360 - learning_rate: 5.0000e-04\n",
            "Epoch 26/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 90ms/step - loss: 39536.7031 - mae: 152.3945 - val_loss: 25256.3438 - val_mae: 118.9831 - learning_rate: 5.0000e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 91ms/step - loss: 41958.8164 - mae: 155.9656 - val_loss: 25352.3125 - val_mae: 121.6627 - learning_rate: 5.0000e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - loss: 43389.4844 - mae: 160.7547 - val_loss: 27827.2207 - val_mae: 127.8749 - learning_rate: 5.0000e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 94ms/step - loss: 43240.4453 - mae: 157.7170 - val_loss: 196315.7344 - val_mae: 122.2595 - learning_rate: 5.0000e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 92ms/step - loss: 37017.6328 - mae: 147.6963 - val_loss: 205315.9531 - val_mae: 120.5052 - learning_rate: 2.5000e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 93ms/step - loss: 34842.8555 - mae: 143.5492 - val_loss: 101423.8984 - val_mae: 118.1530 - learning_rate: 2.5000e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 91ms/step - loss: 32451.4355 - mae: 138.7922 - val_loss: 118409.3750 - val_mae: 116.0570 - learning_rate: 2.5000e-04\n",
            "Epoch 33/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 94ms/step - loss: 30447.2402 - mae: 134.9536 - val_loss: 22375.0586 - val_mae: 108.8623 - learning_rate: 2.5000e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 90ms/step - loss: 31143.8105 - mae: 137.3045 - val_loss: 73846.3750 - val_mae: 113.0384 - learning_rate: 2.5000e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 92ms/step - loss: 32140.1914 - mae: 137.7961 - val_loss: 21822.2500 - val_mae: 108.3147 - learning_rate: 2.5000e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 98ms/step - loss: 29724.5527 - mae: 132.7242 - val_loss: 21953.1289 - val_mae: 104.7797 - learning_rate: 2.5000e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 91ms/step - loss: 29892.2266 - mae: 132.8850 - val_loss: 20433.3086 - val_mae: 103.3968 - learning_rate: 2.5000e-04\n",
            "Epoch 38/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 97ms/step - loss: 27583.9941 - mae: 127.1403 - val_loss: 23095.7051 - val_mae: 102.5563 - learning_rate: 2.5000e-04\n",
            "Epoch 39/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 90ms/step - loss: 26663.0000 - mae: 124.4698 - val_loss: 19955.8047 - val_mae: 101.3702 - learning_rate: 2.5000e-04\n",
            "Epoch 40/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 95ms/step - loss: 27814.4688 - mae: 127.2905 - val_loss: 18798.3340 - val_mae: 99.2047 - learning_rate: 2.5000e-04\n",
            "Epoch 41/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 94ms/step - loss: 24308.2480 - mae: 119.1935 - val_loss: 19973.1211 - val_mae: 102.1576 - learning_rate: 2.5000e-04\n",
            "Epoch 42/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 99ms/step - loss: 24226.8027 - mae: 119.2113 - val_loss: 18828.7402 - val_mae: 99.8830 - learning_rate: 2.5000e-04\n",
            "Epoch 43/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 93ms/step - loss: 23128.2578 - mae: 117.1475 - val_loss: 18590.3320 - val_mae: 98.0532 - learning_rate: 2.5000e-04\n",
            "Epoch 44/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 101ms/step - loss: 21171.0254 - mae: 111.4940 - val_loss: 18342.1211 - val_mae: 96.0867 - learning_rate: 2.5000e-04\n",
            "Epoch 45/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 93ms/step - loss: 21892.7910 - mae: 113.7900 - val_loss: 17816.9961 - val_mae: 94.2827 - learning_rate: 2.5000e-04\n",
            "Epoch 46/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 90ms/step - loss: 21013.1504 - mae: 110.7430 - val_loss: 17814.4980 - val_mae: 91.9879 - learning_rate: 2.5000e-04\n",
            "Epoch 47/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 94ms/step - loss: 19693.5566 - mae: 107.5987 - val_loss: 17167.8789 - val_mae: 93.4717 - learning_rate: 2.5000e-04\n",
            "Epoch 48/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 93ms/step - loss: 19531.3848 - mae: 106.9794 - val_loss: 16122.7412 - val_mae: 90.4278 - learning_rate: 2.5000e-04\n",
            "Epoch 49/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 93ms/step - loss: 20278.9395 - mae: 109.1167 - val_loss: 16498.3301 - val_mae: 89.8100 - learning_rate: 2.5000e-04\n",
            "Epoch 50/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 92ms/step - loss: 20178.0762 - mae: 109.0466 - val_loss: 15988.6582 - val_mae: 88.4262 - learning_rate: 2.5000e-04\n",
            "Epoch 51/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 91ms/step - loss: 19978.4668 - mae: 108.3007 - val_loss: 17530.6230 - val_mae: 91.3741 - learning_rate: 2.5000e-04\n",
            "Epoch 52/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 93ms/step - loss: 19011.6074 - mae: 106.3872 - val_loss: 16248.0332 - val_mae: 89.7226 - learning_rate: 2.5000e-04\n",
            "Epoch 53/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 91ms/step - loss: 19525.8320 - mae: 106.7708 - val_loss: 15611.1875 - val_mae: 87.7101 - learning_rate: 2.5000e-04\n",
            "Epoch 54/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 99ms/step - loss: 19459.4980 - mae: 106.8175 - val_loss: 20443.5527 - val_mae: 107.5249 - learning_rate: 2.5000e-04\n",
            "Epoch 55/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 93ms/step - loss: 18950.7324 - mae: 105.6507 - val_loss: 16677.7305 - val_mae: 90.2316 - learning_rate: 2.5000e-04\n",
            "Epoch 56/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 90ms/step - loss: 16926.1426 - mae: 99.6768 - val_loss: 16200.7285 - val_mae: 87.7920 - learning_rate: 2.5000e-04\n",
            "Epoch 57/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 92ms/step - loss: 17721.8184 - mae: 101.7731 - val_loss: 16228.3535 - val_mae: 87.2655 - learning_rate: 2.5000e-04\n",
            "Epoch 58/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 99ms/step - loss: 18255.0273 - mae: 104.6208 - val_loss: 19269.3574 - val_mae: 93.4695 - learning_rate: 2.5000e-04\n",
            "Epoch 59/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 93ms/step - loss: 18232.5020 - mae: 103.8311 - val_loss: 15606.4707 - val_mae: 86.4702 - learning_rate: 1.2500e-04\n",
            "Epoch 60/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 91ms/step - loss: 15714.0381 - mae: 95.5335 - val_loss: 15859.5225 - val_mae: 87.4889 - learning_rate: 1.2500e-04\n",
            "Epoch 61/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 92ms/step - loss: 15682.2676 - mae: 96.7808 - val_loss: 16825.9883 - val_mae: 89.7889 - learning_rate: 1.2500e-04\n",
            "Epoch 62/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 98ms/step - loss: 16870.6855 - mae: 100.2118 - val_loss: 16438.2188 - val_mae: 89.4424 - learning_rate: 1.2500e-04\n",
            "Epoch 63/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 91ms/step - loss: 17557.8320 - mae: 102.6228 - val_loss: 15973.5098 - val_mae: 89.2683 - learning_rate: 1.2500e-04\n",
            "Epoch 64/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 93ms/step - loss: 17202.9375 - mae: 101.0309 - val_loss: 17588.2812 - val_mae: 91.1705 - learning_rate: 1.2500e-04\n",
            "Epoch 65/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 92ms/step - loss: 15531.7100 - mae: 95.8872 - val_loss: 17363.3867 - val_mae: 91.5059 - learning_rate: 6.2500e-05\n",
            "Epoch 66/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 93ms/step - loss: 16790.1504 - mae: 100.4174 - val_loss: 16788.1074 - val_mae: 89.9486 - learning_rate: 6.2500e-05\n",
            "Epoch 67/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 92ms/step - loss: 15651.3408 - mae: 96.0763 - val_loss: 17125.7930 - val_mae: 90.4191 - learning_rate: 6.2500e-05\n",
            "Epoch 68/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 93ms/step - loss: 14910.6201 - mae: 93.8924 - val_loss: 16696.8047 - val_mae: 90.1106 - learning_rate: 6.2500e-05\n",
            "Epoch 69/100\n",
            "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 91ms/step - loss: 15601.3457 - mae: 95.8681 - val_loss: 17455.1777 - val_mae: 90.1518 - learning_rate: 6.2500e-05\n",
            "\u001b[1m108/108\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step\n",
            "Saving models...\n",
            "Models saved successfully in saved_models/20250212_202905\n",
            "Models saved to: saved_models/20250212_202905\n",
            "\n",
            "Model Performance Metrics:\n",
            "\n",
            "LINEAR Model:\n",
            "RMSE: 101.2496\n",
            "MAE: 77.8870\n",
            "R2: 0.9412\n",
            "\n",
            "RF Model:\n",
            "RMSE: 70.4292\n",
            "MAE: 46.3035\n",
            "R2: 0.9715\n",
            "\n",
            "GBM Model:\n",
            "RMSE: 70.4158\n",
            "MAE: 48.9103\n",
            "R2: 0.9715\n",
            "\n",
            "XGB Model:\n",
            "RMSE: 68.5109\n",
            "MAE: 45.8746\n",
            "R2: 0.9731\n",
            "\n",
            "LSTM Model:\n",
            "RMSE: 124.9259\n",
            "MAE: 86.4702\n",
            "R2: 0.9105\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    print(\"Loading data...\")\n",
        "    train_df, test_df = load_data('training.xlsx', 'testing.xlsx')\n",
        "\n",
        "    print(\"Creating datetime features...\")\n",
        "    train_df = create_datetime_features(train_df)\n",
        "    test_df = create_datetime_features(test_df)\n",
        "\n",
        "    print(\"Performing EDA...\")\n",
        "    perform_eda(train_df)\n",
        "\n",
        "    print(\"Preparing features...\")\n",
        "    train_df = prepare_features(train_df, is_training=True)\n",
        "    test_df = prepare_features(test_df, is_training=False)\n",
        "\n",
        "    # Sort the dataframe by datetime to ensure proper sequence creation\n",
        "    train_df = train_df.sort_values('datetime')\n",
        "\n",
        "    # Calculate the split point\n",
        "    train_size = int(len(train_df) * 0.8)\n",
        "\n",
        "    # Prepare features for modeling\n",
        "    feature_cols = [col for col in train_df.columns if col not in\n",
        "                   ['datetime', 'Load', 'Year', 'Month', 'Day', 'Hour', 'season']]\n",
        "\n",
        "    # Create X and y before splitting\n",
        "    X = train_df[feature_cols]\n",
        "    y = train_df['Load']\n",
        "\n",
        "    # Split the data\n",
        "    X_train = X[:train_size]\n",
        "    y_train = y[:train_size]\n",
        "    X_val = X[train_size:]\n",
        "    y_val = y[train_size:]\n",
        "\n",
        "    # Ensure indices are reset and aligned\n",
        "    X_train = X_train.reset_index(drop=True)\n",
        "    y_train = y_train.reset_index(drop=True)\n",
        "    X_val = X_val.reset_index(drop=True)\n",
        "    y_val = y_val.reset_index(drop=True)\n",
        "\n",
        "    print(f\"Training set shapes - X: {X_train.shape}, y: {y_train.shape}\")\n",
        "    print(f\"Validation set shapes - X: {X_val.shape}, y: {y_val.shape}\")\n",
        "\n",
        "    print(\"Training models...\")\n",
        "    forecaster = LoadForecaster()\n",
        "    results = forecaster.train_evaluate_models(X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Save the trained models\n",
        "    print(\"Saving models...\")\n",
        "    model_path = save_trained_models(forecaster)\n",
        "    print(f\"Models saved to: {model_path}\")\n",
        "\n",
        "    print(\"\\nModel Performance Metrics:\")\n",
        "    for model_name, metrics in results.items():\n",
        "        print(f\"\\n{model_name.upper()} Model:\")\n",
        "        for metric_name, value in metrics.items():\n",
        "            print(f\"{metric_name}: {value:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eCosKVRo5PHR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}